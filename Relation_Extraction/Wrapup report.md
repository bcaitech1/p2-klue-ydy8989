# Wrapup report

**순위: ** LB 79.9% / 46등으로 최종 마무리



### 이번 대회의 목표

1. 지난 대회 부족한 점 보완하기
	1. '정상적'으로 실험하기 : 변인통제
2. baseline code 사용하지 않기(최대한)



### 최종 제출 모델 및 방법

- Stratified KFold validation 기준 가장 성능 좋은 best 5를 앙상블한 모델
- 앙상블한 모델 5개:
	- Roberta-large (batch 4) 2개 + (batch 8) 3개 - 시드 변경
- 공통 hyperparameter:
	- optimizer : AdamW
	- scheduler : StepLR - gamma:0.794
	- learning rate : 1e-5
	- cost fn : Labelsmooth loss



### 시도했었던 방법들

- 모델 변경:

	- 사용해봤던 architecture는 `bert-base-multilingual-cased` - `koelectra-base` - `kobert` - `kobart` - `roberta-large` 순으로 사용해봤습니다. 

	- 일반적으로 multilingual 모델이 kobert와 같은 한글 pretrained 모델보다 동일 조건에서 더 좋은 성능을 보였습니다. 시간이 부족해서 전략적으로 fine tuning을 더 꼼꼼하게 못해서 그랬을 수도 있다고 생각했습니다.

		

- 데이터 증강 및 전처리:

	- EDA(Easy Data Augmentation):
		- 문장에 단어를 유의어로 교체(SR), 문장에 단어를 랜덤 삽입(RI), 문장의 단어를 랜덤 교체(RS), 문장의 단어를 랜덤 삭제(RD) 중 RS와 RD를 사용했습니다.
		- 데이터가 많지 않을 경우 RS와 RD만을 사용하는 것을 추천했기에 이와 같이 사용했습니다.
		- 하지만 큰 성능 개선 효과는 보지 못했습니다.
	- Back Translation: 
		- 한글 - 영어 - 한글 순으로 데이터를 증강하여 사용하였고, 성능 개선의 효과는 제대로 검증하지 못했습니다. 
		- 그 이유는 검증에 어려움이 있었기 때문입니다. stratified kfold 사용시 validation set에서 약 92퍼센트의 성능을 달성하였지만, 제출 시 높지 않은 성능을 보였는데 이 원인은 validation set의 분포와 다르기 때문으로 인지하였습니다. 
		- 따라서 추후에 어느정도 하이퍼파라미터가 고정된 후 사용하려 하였지만, 시간 관계상 재시도하지 못한 채 끝났습니다. 
	- TEM(Typed Entity Marker):
		- pororo를 활용한 개체명 앞뒤로 마킹을 해주는 방식을 사용해 봤습니다. 
		- 해당 방식도 동일 조건 뚜렷한 성능 향상을 보이지 않아 한 번 시도해보고 미뤄뒀었습니다. 

- Loss function:

	- 최종적으로는 labelsmooth loss를 사용했습니다. 
	- 크로스 엔트로피 - 커스텀 로스(CE + F1loss) - focal 순으로 사용해보았지만 그 중 labelsmooth loss가 가장 좋은 성능을 보였습니다. 
	- imbalance 문제로 인해 focal이 가장 좋을 것으로 생각했지만, tensorboard 상에서 validation acc의 모양이 다소 불안정하고 좋지 않았기에 사용하지 않았습니다. 

- scheduler:

	- CustomCosineAnnealingWarmUpRestarts - cycleLR - stepLR 등을 사용해보았습니다. 
	- 시도했던 모델 별로 적절한 learning rate가 달랐고, 초반에 찾기 힘들었기 때문에 이를 조정하기 위해 사용했습니다. 
	- stepLR과 달리 두 스케줄러는 주기적으로 learning rate를 움직이는 스케줄러였기에 실험 환경에서 다루기 힘들다는 생각이 들었습니다. 후반부까지 stepLR를 사용하다가 바꾸려는 생각이었지만, 그러지 못한 채 마무리되었습니다. 



### 시도하려 했으나 못했던 방법들

- `관계없음(0)` 레이블과 `나머지(1~41)`를 분류한 multi label 모델
	- 이번 대회의 데이터에서 관계 없음과 나머지를 분류하는 것이 중요하다고 생각했습니다. 
	- 문장 내 두 entity가 특정한 관계로 분류되는 것과 그렇지 않은 것은 많은 차이가 있기 때문입니다. 
	- 1~41의 클래스에 해당하는 엔터티들은 유의미한 관계이지만, 클래스 0에 속하는 두 엔터티는 그야말로 '아무런' 관계도 없기 때문입니다. 바꿔 말하면 문장내에서 아무 두 엔터티를 뽑는 것과 같은 관계이기 때문입니다.
	- 그렇기에 0과 1~41을 먼저 분류하고, 이후 41개의 클래스로 분류하는 것이 효과적일 것으로 생각했습니다. 
	- 시도하지 못한 이유 : 시간적 여유가 없었습니다. GPU를 사용하지 못하는 이슈를 4일간 해결하지 못하였기에 정상적으로 실험하는 기간이 짧았습니다. 



### 아쉬운점

- 서버 이슈:

	- 학습 속도가 매우 느려 확인해보니 cpu로 작동하고 있었고, 이를 해결하는데 약 3~4일이 걸렸습니다. 베이스라인을 구현하는 데 어려움은 없었지만, 실험환경을 구축하기까지 너무 많은 시간이 소모되어 아쉬웠습니다. 
	- 특히, 용량을 공유하는 점은 매우 아쉬웠습니다. 

- 실험 및 기술 관련:

	- 학습을 하면서 배치사이즈를 낮췄을 때 더 좋은 성능을 보였습니다. 
	- 또한, pretrained model을 사용하다보니 에폭을 늘릴 수 없었고(overfitting), 배치 사이즈도 낮았기에 한 에폭당 스텝 수가 늘어났습니다. 
	- 특히, 스케줄러를 사용하였기에 learning rate가 조금씩 낮아졌는데, 이에 반해 많은 step을 거친 후에나 한 번씩 learning rate가 갱신되었기에 최적의 값을 찾는데 다소 어려움이 있었던 것 같습니다. 
	- 또한 에폭당 evaluation을 한 번씩 수행하게 진행하면서 eval acc이 최대일 때 모델을 저장하게 진행했었는데 에폭 단위가 아닌 스텝 단위로 evaluation을 진행했다면 80%의 정확도를 달성할 수 있었을 것으로 생각합니다. 

- 데이터 관점:

	- 다양한 데이터 전처리 및 기법들을 사용하지 못한 점이 아쉽습니다. 

	- 서버 이슈로 모델 선정 및 fine tuning이 많이 늦어졌고, 더 깊이 데이터 depth에서 처리하지 못한 점이 아쉽습니다. 

	- 특히, 제대로 실험하지 못한 위 세가지 방법들(EDA, Back Translation, TEM)에 대한 모델 튜닝을 통해 더 성능 개선의 여지가 있는지 확인하지 못한 점이 아쉽습니다. 

		

### 대회 중 얻은 인사이트

- Learning rate와 batch size의 중요성:
	- 기존까지는 learning rate와 배치사이즈는 학습 정확도에 큰 영향을 주지 않는다고 생각했었습니다. 
	- 두 가지를 조절해야하는 이유는 단지 학습 속도에 영향을 미치는 하이퍼파라미터이고, 더 빠른 학습 속도로 더 많은 실험을 한다는 점에 의의가 있다고 별 의심없이 받아들였습니다. 
	- 하지만 이번 기회에 배치 사이즈는 최적해에 도달하기까지 조금은 느리지만 더 정확하게 도달하는 방법임을 깨달았던 좋은 경험이었습니다.
- validation set의 중요성:
	- (train set과 test set의 분포가 비슷하다는 가정하에) 적절한 validation set의 구성은 실험 환경에 대한 믿음을 준다는 것을 경험했습니다. 



### 끝맺음

- 지난 스테이지에서의 부족했던 부분을 보완하기 위해 노력했습니다. 

	> 변인 통제후 정상적인 실험환경 구성을 통한 실험의 진전
	>
	> 개발 환경 구현에 대한 최적화

- 어느정도는 지켰다고 생각하지만, 아직 많은 부분이 부족하다고 생각합니다. 서버 이슈로 다급했었기에 확장성 좋은 코드를 구현하지 못했고, 이는 생산성을 극대화 시키지 못하는 결과로 이어졌다고 생각합니다. 
- **끝으로 궁금한 점은, 실험을 진행할 때 어떠한 순서로 진행해야하는지 궁금합니다. 모델을 선정하고 새로운 방법들을 추가할 때마다 그에 맞게 다시 tuning 해주어야한다고 생각하는데, 이를 어떠한 순서로 관리해야 할 지 궁금합니다. **

